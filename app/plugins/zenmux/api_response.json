{"success":true,"data":[{"id":"2533MOqY6iwA14613666","name":"Anthropic: Claude Sonnet 4","description":"Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.","pricing_completion":"15","pricing_prompt":"3","author":"anthropic","slug":"anthropic/claude-sonnet-4","supported_parameters":"max_completion_tokens,temperature,stop,tools,tool_choice,top_p,parallel_tool_calls,max_tokens","display_endpoint_id":"2533EPIjUXJ214613666","input_modalities":"image,text,file","context_length":1000000,"token_week":"72925121","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/Tt3vhMJ/Property-1bedrock.svg"],"uptimeHh":{"2025091005":1,"2025091006":0.976378,"2025091007":0.944223,"2025091008":0.987395,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":0.75,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":0.724696,"2025091210":0.878049,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":0.587912,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOp1WAr314615537","name":"Anthropic: Claude 3.7 Sonnet","description":"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.","pricing_completion":"15","pricing_prompt":"3","author":"anthropic","slug":"anthropic/claude-3.7-sonnet","supported_parameters":"temperature,stop,tools,tool_choice,max_completion_tokens,top_p,max_tokens","display_endpoint_id":"2533EPIjUXJ214613666","input_modalities":"text,image,file","context_length":200000,"token_week":"36860549","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/Tt3vhMJ/Property-1bedrock.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MO1drqd114615544","name":"OpenAI: GPT-4.1 Mini","description":"GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lower latency and cost. It retains a 1 million token context window and scores 45.1% on hard instruction evals, 35.8% on MultiChallenge, and 84.1% on IFEval. Mini also shows strong coding ability (e.g., 31.6% on Aider’s polyglot diff benchmark) and vision understanding, making it suitable for interactive applications with tight performance constraints.","pricing_completion":"1.6","pricing_prompt":"0.4","author":"openai","slug":"openai/gpt-4.1-mini","supported_parameters":"tools,tool_choice,seed,response_format,temperature,top_p,stop,frequency_penalty,presence_penalty,logit_bias,logprobs,top_logprobs,max_completion_tokens","display_endpoint_id":"2533EPuuKR5z14613667","input_modalities":"image,text,file","context_length":1047576,"token_week":"8618783","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/EpLeHOc/Property-1Azure.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2537MOR6Qy8U14621507","name":"MoonshotAI: Kimi K2 0905","description":"Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k.\n\nThis update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.","pricing_completion":"2.5","pricing_prompt":"0.6","author":"moonshotai","slug":"moonshotai/kimi-k2-0905","supported_parameters":"tools,response_format,max_tokens,temperature,top_p,stop,frequency_penalty,presence_penalty","display_endpoint_id":"2537EPiPqPC014624665","input_modalities":"text","context_length":256000,"token_week":"7060201","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/zK3cn57/Property-1moonshot.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/zK3cn57/Property-1moonshot.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":0.833333,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2534MOg92srJ14617117","name":"Google: Gemini 2.5 Pro","description":"Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.","pricing_completion":"10","pricing_prompt":"1.25","author":"google","slug":"google/gemini-2.5-pro","supported_parameters":"max_completion_tokens,temperature,stop,tools,tool_choice,seed,top_p","display_endpoint_id":"2533EPCKe7Sf14615305","input_modalities":"file,image,text,audio","context_length":1048576,"token_week":"4367616","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/f98BGHB/Property-1Google.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":0.997175,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOA9v8lt14613671","name":"OpenAI: GPT-5","description":"GPT-5 is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.","pricing_completion":"10","pricing_prompt":"1.25","author":"openai","slug":"openai/gpt-5","supported_parameters":"tools,tool_choice,response_format,max_completion_tokens","display_endpoint_id":"2533EPLSSsqK14613665","input_modalities":"text,image,file","context_length":400000,"token_week":"3404379","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/EpLeHOc/Property-1Azure.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOgC0uoW14615543","name":"OpenAI: GPT-4.1","description":"GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.","pricing_completion":"8","pricing_prompt":"2","author":"openai","slug":"openai/gpt-4.1","supported_parameters":"tools,tool_choice,seed,response_format,temperature,top_p,stop,frequency_penalty,presence_penalty,logit_bias,logprobs,top_logprobs,max_completion_tokens","display_endpoint_id":"2533EPuuKR5z14613667","input_modalities":"image,text,file","context_length":1047576,"token_week":"3005999","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/EpLeHOc/Property-1Azure.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2534MOEMmdUf14617115","name":"Google: Gemini 2.5 Flash","description":"Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation.","pricing_completion":"2.5","pricing_prompt":"0.3","author":"google","slug":"google/gemini-2.5-flash","supported_parameters":"max_completion_tokens,temperature,tools,tool_choice,stop,seed,top_p","display_endpoint_id":"2533EPCKe7Sf14615305","input_modalities":"file,image,text,audio","context_length":1048576,"token_week":"2832173","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/f98BGHB/Property-1Google.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":0.8,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":0.992537,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":0.5,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":0.990291,"2025091304":0.990323}},{"id":"2537MOHTrnch14623350","name":"Qwen3-Coder-Plus","description":"Powered by Qwen3, this is a powerful Coding Agent that excels in tool calling and environment interaction to achieve autonomous programming. It combines outstanding coding proficiency with versatile general-purpose abilities.","pricing_completion":"5","pricing_prompt":"1","author":"qwen","slug":"qwen/qwen3-coder-plus","supported_parameters":"top_p,presence_penalty,response_format,max_completion_tokens,max_tokens,seed,logprobs,top_logprobs,stop,tool_choice,parallel_tool_calls,tools,temperature","display_endpoint_id":"2537EP8X0UQx14624666","input_modalities":"text","context_length":1000000,"token_week":"1839122","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/qua1OIv/Property-1Qwen.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/f9QQZjY/Property-1Alibaba-cloud.svg"],"uptimeHh":{"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2537MOnZMdPV14623347","name":"DeepSeek: deepseek-chat (v3.1)","description":"DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\n\nFor model details, please visit the DeepSeek-V3 repo for more information, or see the launch announcement.","pricing_completion":"1.68","pricing_prompt":"0.56","author":"deepseek","slug":"deepseek/deepseek-chat","supported_parameters":"max_completion_tokens,temperature,top_p,frequency_penalty,presence_penalty,response_format,stop,tool_choice,logprobs,top_logprobs,tools,max_tokens","display_endpoint_id":"2537EPhQMkxV14622829","input_modalities":"text","context_length":128000,"token_week":"1822920","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZYcK21S/Property-1deepseek.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZYcK21S/Property-1deepseek.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOLQKhC514613672","name":"Google: Gemini 2.5 Flash Lite","description":"Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the Reasoning API parameter to selectively trade off cost for intelligence. ","pricing_completion":"0.4","pricing_prompt":"0.1","author":"google","slug":"google/gemini-2.5-flash-lite","supported_parameters":"max_completion_tokens,temperature,stop,tools,tool_choice,seed,top_p","display_endpoint_id":"2533EPCKe7Sf14615305","input_modalities":"file,image,text,audio","context_length":1048576,"token_week":"1732896","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/f98BGHB/Property-1Google.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOqFyroO14615535","name":"Anthropic: Claude Opus 4.1","description":"Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.","pricing_completion":"75","pricing_prompt":"15","author":"anthropic","slug":"anthropic/claude-opus-4.1","supported_parameters":"temperature,stop,tools,tool_choice,max_completion_tokens,top_p,parallel_tool_calls,max_tokens","display_endpoint_id":"2533EPIjUXJ214613666","input_modalities":"image,text,file","context_length":200000,"token_week":"1254695","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/Tt3vhMJ/Property-1bedrock.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MO9jvEwI14615546","name":"OpenAI: o4 Mini","description":"OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.","pricing_completion":"4.4","pricing_prompt":"1.1","author":"openai","slug":"openai/o4-mini","supported_parameters":"tools,tool_choice,response_format,seed,max_completion_tokens","display_endpoint_id":"2533EPuuKR5z14613667","input_modalities":"image,text","context_length":200000,"token_week":"828637","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/EpLeHOc/Property-1Azure.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2535MOpFS4p814620455","name":"Google: Gemini 2.0 Flash Lite","description":"Gemini 2.0 Flash Lite offers a significantly faster time to first token (TTFT) compared to Gemini Flash 1.5, while maintaining quality on par with larger models like Gemini Pro 1.5, all at extremely economical token prices.","pricing_completion":"0.3","pricing_prompt":"0.075","author":"google","slug":"google/gemini-2.0-flash-lite-001","supported_parameters":"max_completion_tokens,temperature,seed,stop,tools,tool_choice,top_p","display_endpoint_id":"2533EPCKe7Sf14615305","input_modalities":"text,image,file,audio","context_length":1048576,"token_week":"529566","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/f98BGHB/Property-1Google.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2534MOERppWt14619339","name":"DeepSeek: DeepSeek V3.1","description":"DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes via prompt templates. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference.\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows.\n\nIt succeeds the DeepSeek V3-0324 model and performs well on a variety of tasks.","pricing_completion":"1.11","pricing_prompt":"0.28","author":"deepseek","slug":"deepseek/deepseek-chat-v3.1","supported_parameters":"max_completion_tokens,temperature,top_p,frequency_penalty,presence_penalty,seed,response_format,stop,tools,tool_choice","display_endpoint_id":"2533EP7brvqz14615306","input_modalities":"text","context_length":128000,"token_week":"459329","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZYcK21S/Property-1deepseek.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZjbvtDO/Property-1Theta.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/tkJqAng/Property-1huoshan.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2534MOhFFqQw14617119","name":"MoonshotAI: Kimi K2","description":"Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.","pricing_completion":"2.23","pricing_prompt":"0.56","author":"/moonshotai","slug":"moonshotai/kimi-k2","supported_parameters":"max_completion_tokens,temperature,top_p,frequency_penalty,presence_penalty,stop,response_format,tools,parallel_tool_calls","display_endpoint_id":"2534EP3br5ma14617867","input_modalities":"text","context_length":128000,"token_week":"402855","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/zK3cn57/Property-1moonshot.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZjbvtDO/Property-1Theta.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/tkJqAng/Property-1huoshan.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2534MOKFvye414617123","name":"Google: Gemini 2.0 Flash","description":"Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to Gemini Flash 1.5, while maintaining quality on par with larger models like Gemini Pro 1.5. It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.","pricing_completion":"0.4","pricing_prompt":"0.1","author":"google","slug":"google/gemini-2.0-flash","supported_parameters":"max_completion_tokens,temperature,stop,tools,tool_choice,top_p,seed","display_endpoint_id":"2533EPCKe7Sf14615305","input_modalities":"text,image,file,audio","context_length":1048576,"token_week":"370720","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/f98BGHB/Property-1Google.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOxJJWOJ14613670","name":"OpenAI: GPT-5 Chat","description":"GPT-5 Chat is designed for advanced, natural, multimodal, and context-aware conversations for enterprise applications.","pricing_completion":"10","pricing_prompt":"1.25","author":"openai","slug":"openai/gpt-5-chat","supported_parameters":"response_format,seed,max_tokens","display_endpoint_id":"2533EPuuKR5z14613667","input_modalities":"file,image,text","context_length":128000,"token_week":"286320","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/EpLeHOc/Property-1Azure.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOBz6vKU14615540","name":"OpenAI: GPT-4o","description":"GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal","pricing_completion":"10","pricing_prompt":"2.5","author":"openai","slug":"openai/gpt-4o","supported_parameters":"tools,tool_choice,seed,response_format,temperature,top_p,stop,frequency_penalty,presence_penalty,logit_bias,logprobs,top_logprobs,max_completion_tokens","display_endpoint_id":"2533EPuuKR5z14613667","input_modalities":"text,image,file","context_length":128000,"token_week":"242808","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/EpLeHOc/Property-1Azure.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOLObjQe14615538","name":"Anthropic: Claude 3.5 Haiku","description":"Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\n\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.","pricing_completion":"4","pricing_prompt":"0.8","author":"anthropic","slug":"anthropic/claude-3.5-haiku","supported_parameters":"tools,tool_choice,temperature,top_p,stop,max_completion_tokens,max_tokens","display_endpoint_id":"2533EPIjUXJ214613666","input_modalities":"text,image","context_length":200000,"token_week":"241360","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/Tt3vhMJ/Property-1bedrock.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOp9eWVK14615541","name":"DeepSeek: R1 0528","description":"May 28th update to the original DeepSeek R1 Performance on par with OpenAI o1, but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\nFully open-source model.","pricing_completion":"2.23","pricing_prompt":"0.56","author":"deepseek","slug":"deepseek/deepseek-r1-0528","supported_parameters":"temperature,top_p,frequency_penalty,presence_penalty,max_completion_tokens,tools,stop,tool_choice,seed,logprobs,response_format","display_endpoint_id":"2533EP7brvqz14615306","input_modalities":"text","context_length":64000,"token_week":"224339","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZYcK21S/Property-1deepseek.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZjbvtDO/Property-1Theta.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/tkJqAng/Property-1huoshan.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2537MOtACfWn14623345","name":"Z.AI: GLM 4.5","description":"GLM-4.5 is our latest flagship foundation model, purpose-built for agent-based applications. It leverages a Mixture-of-Experts (MoE) architecture and supports a context length of up to 128k tokens. GLM-4.5 delivers significantly enhanced capabilities in reasoning, code generation, and agent alignment. It supports a hybrid inference mode with two options, a \"thinking mode\" designed for complex reasoning and tool use, and a \"non-thinking mode\" optimized for instant responses. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)","pricing_completion":"1.54","pricing_prompt":"0.35","author":"z-ai","slug":"z-ai/glm-4.5","supported_parameters":"max_completion_tokens,temperature,top_p,stop,tools,tool_choice","display_endpoint_id":"2537EPXqYt3314622828","input_modalities":"text","context_length":128000,"token_week":"190913","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/swD1Pg4/Property-1Zai.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/swD1Pg4/Property-1Zai.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOfTfgfG14615542","name":"OpenAI: GPT-5 Nano","description":"GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications.","pricing_completion":"0.4","pricing_prompt":"0.05","author":"openai","slug":"openai/gpt-5-nano","supported_parameters":"tools,tool_choice,response_format,max_completion_tokens","display_endpoint_id":"2533EPLSSsqK14613665","input_modalities":"text,image,file","context_length":400000,"token_week":"185399","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/EpLeHOc/Property-1Azure.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2534MOYgYrGv14617121","name":"Qwen: Qwen3 235B A22B Thinking 2507","description":"Qwen3-235B-A22B-Thinking-2507 is a high-performance, open-weight Mixture-of-Experts (MoE) language model optimized for complex reasoning tasks. It activates 22B of its 235B parameters per forward pass and natively supports up to 262,144 tokens of context. This \"thinking-only\" variant enhances structured logical reasoning, mathematics, science, and long-form generation, showing strong benchmark performance across AIME, SuperGPQA, LiveCodeBench, and MMLU-Redux. It enforces a special reasoning mode (</think>) and is designed for high-token outputs (up to 81,920 tokens) in challenging domains.\n\nThe model is instruction-tuned and excels at step-by-step reasoning, tool use, agentic workflows, and multilingual tasks. This release represents the most capable open-source variant in the Qwen3-235B series, surpassing many closed models in structured reasoning use cases.","pricing_completion":"2.78","pricing_prompt":"0.28","author":"qwen","slug":"qwen/qwen3-235b-a22b-thinking-2507","supported_parameters":"max_completion_tokens,temperature,top_p,frequency_penalty,presence_penalty,seed,logprobs,response_format,stop,tools,tool_choice","display_endpoint_id":"2533EP7brvqz14615306","input_modalities":"text","context_length":256000,"token_week":"181910","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/qua1OIv/Property-1Qwen.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZjbvtDO/Property-1Theta.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":0.8,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2534MOQ0mq8V14619335","name":"OpenAI: GPT-4o-mini","description":"GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\n#multimodal","pricing_completion":"0.6","pricing_prompt":"0.15","author":"openai","slug":"openai/gpt-4o-mini","supported_parameters":"tools,tool_choice,seed,response_format,temperature,top_p,stop,frequency_penalty,presence_penalty,logit_bias,logprobs,top_logprobs,max_completion_tokens","display_endpoint_id":"2533EPuuKR5z14613667","input_modalities":"text,image,file","context_length":128000,"token_week":"111796","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/EpLeHOc/Property-1Azure.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MO4sI4zB14615536","name":"Anthropic: Claude Opus 4","description":"Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. ","pricing_completion":"75","pricing_prompt":"15","author":"anthropic","slug":"anthropic/claude-opus-4","supported_parameters":"temperature,stop,tools,tool_choice,max_completion_tokens,top_p,parallel_tool_calls,max_tokens","display_endpoint_id":"2533EPIjUXJ214613666","input_modalities":"image,text,file","context_length":200000,"token_week":"92872","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/Tt3vhMJ/Property-1bedrock.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":0.777778,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOrAdpdf14613669","name":"OpenAI: GPT-5 Mini","description":"GPT-5 Mini is a compact version of GPT-5, designed to handle lighter-weight reasoning tasks. It provides the same instruction-following and safety-tuning benefits as GPT-5, but with reduced latency and cost. GPT-5 Mini is the successor to OpenAI's o4-mini model.","pricing_completion":"2","pricing_prompt":"0.25","author":"openai","slug":"openai/gpt-5-mini","supported_parameters":"tools,tool_choice,response_format,max_completion_tokens","display_endpoint_id":"2533EPLSSsqK14613665","input_modalities":"text,image,file","context_length":400000,"token_week":"70583","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/EpLeHOc/Property-1Azure.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2537MO5jA0rt14623346","name":"Z.AI: GLM 4.5 Air","description":"GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a \"thinking mode\" for advanced reasoning and tool use, and a \"non-thinking mode\" for real-time interaction. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)","pricing_completion":"0.56","pricing_prompt":"0.11","author":"z-ai","slug":"z-ai/glm-4.5-air","supported_parameters":"max_completion_tokens,temperature,top_p,stop,tools,tool_choice","display_endpoint_id":"2537EPXqYt3314622828","input_modalities":"text","context_length":128000,"token_week":"29378","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/swD1Pg4/Property-1Zai.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/swD1Pg4/Property-1Zai.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2537MOY3n1ZL14621506","name":"MoonshotAI: Kimi K2 0711","description":"Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.","pricing_completion":"2.23","pricing_prompt":"0.56","author":"moonshotai","slug":"moonshotai/kimi-k2-0711","supported_parameters":"max_completion_tokens,temperature,top_p,frequency_penalty,presence_penalty,stop,tools,parallel_tool_calls","display_endpoint_id":"2534EP3br5ma14617867","input_modalities":"text","context_length":128000,"token_week":"11972","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/zK3cn57/Property-1moonshot.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZjbvtDO/Property-1Theta.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/tkJqAng/Property-1huoshan.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2534MOomTK8q14617120","name":"Qwen: Qwen3 235B A22B Instruct 2507","description":"Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks).\n\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.","pricing_completion":"1.11","pricing_prompt":"0.28","author":"qwen","slug":"qwen/qwen3-235b-a22b-2507","supported_parameters":"max_completion_tokens,temperature,top_p,frequency_penalty,presence_penalty,seed,logprobs,response_format,tools,tool_choice,stop","display_endpoint_id":"2533EP7brvqz14615306","input_modalities":"text","context_length":256000,"token_week":"11069","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/qua1OIv/Property-1Qwen.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZjbvtDO/Property-1Theta.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2534MOzPMdNr14617122","name":"Qwen: Qwen3 Coder","description":"Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.","pricing_completion":"5.01","pricing_prompt":"1.25","author":"qwen","slug":"qwen/qwen3-coder","supported_parameters":"max_completion_tokens,temperature,top_p,frequency_penalty,seed,presence_penalty,logit_bias,logprobs,stop,tools,tool_choice,response_format","display_endpoint_id":"2533EP7brvqz14615306","input_modalities":"text","context_length":256000,"token_week":"10748","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/qua1OIv/Property-1Qwen.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/ZjbvtDO/Property-1Theta.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MOHFsqRf14615539","name":"Anthropic: Claude 3.5 Sonnet","description":"New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Scores ~49% on SWE-Bench Verified, higher than the last best score, and without any fancy prompt scaffolding\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)","pricing_completion":"15","pricing_prompt":"3","author":"anthropic","slug":"anthropic/claude-3.5-sonnet","supported_parameters":"tools,tool_choice,temperature,top_p,stop,max_completion_tokens,max_tokens","display_endpoint_id":"2533EPIjUXJ214613666","input_modalities":"text,image,file","context_length":200000,"token_week":"10426","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/0WbxNDF/Property-1vertexai.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/Tt3vhMJ/Property-1bedrock.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2534MOfIdQ7u14619337","name":"Anthropic: Claude 3 Haiku","description":"Claude 3 Haiku is Anthropic's fastest and most compact model for\nnear-instant responsiveness. Quick and accurate targeted performance.","pricing_completion":"1.25","pricing_prompt":"0.25","author":"anthropic","slug":"anthropic/claude-3-haiku","supported_parameters":"tools,tool_choice,temperature,top_p,stop,max_completion_tokens,max_tokens","display_endpoint_id":"2533EPIjUXJ214613666","input_modalities":"text,image","context_length":200000,"token_week":"9995","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dOvpSKZ/Property-1Anthropic.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":0.8,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":0.5,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":0.75,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}},{"id":"2533MO9JCfc514615545","name":"OpenAI: GPT-4.1 Nano","description":"For tasks that demand low latency, GPT‑4.1 nano is the fastest and cheapest model in the GPT-4.1 series. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding – even higher than GPT‑4o mini. It’s ideal for tasks like classification or autocompletion.","pricing_completion":"0.4","pricing_prompt":"0.1","author":"openai","slug":"openai/gpt-4.1-nano","supported_parameters":"tools,tool_choice,seed,response_format,temperature,top_p,stop,frequency_penalty,presence_penalty,logit_bias,logprobs,top_logprobs,max_completion_tokens","display_endpoint_id":"2533EPuuKR5z14613667","input_modalities":"image,text,file","context_length":1047576,"token_week":"6737","iconUrl":"https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","providerIcons":["https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/dX8DUhW/Property-1GPT.svg","https://cdn.marmot-cloud.com/storage/zenmux/2025/09/11/EpLeHOc/Property-1Azure.svg"],"uptimeHh":{"2025091005":1,"2025091006":1,"2025091007":1,"2025091008":1,"2025091009":1,"2025091010":1,"2025091011":1,"2025091012":1,"2025091013":1,"2025091014":1,"2025091015":1,"2025091016":1,"2025091017":1,"2025091018":1,"2025091019":1,"2025091020":1,"2025091021":1,"2025091022":1,"2025091023":1,"2025091100":1,"2025091101":1,"2025091102":1,"2025091103":1,"2025091104":1,"2025091105":1,"2025091106":1,"2025091107":1,"2025091108":1,"2025091109":1,"2025091110":1,"2025091111":1,"2025091112":1,"2025091113":1,"2025091114":1,"2025091115":1,"2025091116":1,"2025091117":1,"2025091118":1,"2025091119":1,"2025091120":1,"2025091121":1,"2025091122":1,"2025091123":1,"2025091200":1,"2025091201":1,"2025091202":1,"2025091203":1,"2025091204":1,"2025091205":1,"2025091206":1,"2025091207":1,"2025091208":1,"2025091209":1,"2025091210":1,"2025091211":1,"2025091212":1,"2025091213":1,"2025091214":1,"2025091215":1,"2025091216":1,"2025091217":1,"2025091218":1,"2025091219":1,"2025091220":1,"2025091221":1,"2025091222":1,"2025091223":1,"2025091300":1,"2025091301":1,"2025091302":1,"2025091303":1,"2025091304":1}}]}