[
  {
    "model": "AiHubmix-Phi-4-mini-reasoning",
    "developer": "Microsoft",
    "developer_id": 3,
    "provider_id": 45,
    "model_name": "Phi-4-mini-reasoning",
    "model_ratio": 0.06,
    "cache_ratio": 1,
    "completion_ratio": 1,
    "img_price_config": "",
    "billing_config": "",
    "desc": "Phi-4-mini-reasoning 是一种轻量级开源模型，旨在进行高级数学推理和逻辑密集型问题解决。它特别适用于形式证明、符号计算以及多步骤文字题的解答。凭借高效的架构，该模型在保持高质量推理性能的同时，实现了成本效益的部署，非常适合教育应用、嵌入式辅导以及轻量级边缘或移动系统。\n\nPhi-4-mini-reasoning 支持128K令牌上下文长度，能够处理和推理长篇数学题目与证明。该模型基于合成和高质量数学数据集，通过监督微调和偏好建模等先进微调技术提升推理能力。其训练过程中融入了安全性与对齐协议，确保在支持的用例中表现稳健可靠。",
    "desc_en": "Phi-4-mini-reasoning is a lightweight open model designed for advanced mathematical reasoning and logic-intensive problem-solving. It is particularly well-suited for tasks such as formal proofs, symbolic computation, and solving multi-step word problems. With its efficient architecture, the model balances high-quality reasoning performance with cost-effective deployment, making it ideal for educational applications, embedded tutoring, and lightweight edge or mobile systems.\n\nPhi-4-mini-reasoning supports a 128K token context length, enabling it to process and reason over long mathematical problems and proofs. Built on synthetic and high-quality math datasets, the model leverages advanced fine-tuning techniques such as supervised fine-tuning and preference modeling to enhance reasoning capabilities. Its training incorporates safety and alignment protocols, ensuring robust and reliable performance across supported use cases.",
    "order": 49,
    "flag": 1,
    "context_length": "",
    "input_images": "",
    "input_audios": "",
    "latency": 0,
    "throughput": 0,
    "usage": "",
    "usage_en": "",
    "modalities": "",
    "features": "thinking",
    "tags": "",
    "types": "t2t",
    "display_input": "",
    "display_output": "",
    "parameters": []
  },
  {
    "model": "AiHubmix-Phi-4-reasoning",
    "developer": "Microsoft",
    "developer_id": 3,
    "provider_id": 8,
    "model_name": "Phi-4-reasoning",
    "model_ratio": 0.1,
    "cache_ratio": 1,
    "completion_ratio": 1,
    "img_price_config": "",
    "billing_config": "",
    "desc": "Phi-4-Reasoning 是一种最先进的开源推理模型，基于 Phi-4 经过监督微调而成，使用链式思维轨迹数据集和强化学习进行微调。监督微调的数据集包括合成提示和来自公共网站的高质量过滤数据，重点涉及数学、科学、编码技能以及安全与负责任人工智能的对齐数据。这种方法的目标是确保训练出具有较强能力的小型模型，其数据专注于高质量和高级推理能力。",
    "desc_en": "Phi-4-Reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. The supervised fine-tuning dataset includes a blend of synthetic prompts and high-quality filtered data from public domain websites, focused on math, science, and coding skills as well as alignment data for safety and Responsible AI. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning.",
    "order": 50,
    "flag": 1,
    "context_length": "",
    "input_images": "",
    "input_audios": "",
    "latency": 0,
    "throughput": 0,
    "usage": "",
    "usage_en": "",
    "modalities": "",
    "features": "thinking",
    "tags": "",
    "types": "t2t",
    "display_input": "",
    "display_output": "",
    "parameters": []
  },
  {
    "model": "AiHubmix-mistral-medium",
    "developer": "Mistral",
    "developer_id": 10,
    "provider_id": 45,
    "model_name": "mistral-medium-2505",
    "model_ratio": 0.2,
    "cache_ratio": 1,
    "completion_ratio": 5,
    "img_price_config": "",
    "billing_config": "",
    "desc": "Mistral Medium 3 是一款最先进且多功能的模型，旨在应对广泛的任务，包括编程、数学推理、理解长文档、摘要和对话。\n\n它具有多模态能力，能够处理视觉输入，并支持包括80多种编码语言在内的数十种语言。此外，它还具备函数调用和自主工作流程。\n\nMistral Medium 3 针对单节点推理进行了优化，特别适用于长上下文应用。其规模使其在单个节点上实现高吞吐量成为可能。",
    "desc_en": "Mistral Medium 3 is a SOTA & versatile model designed for a wide range of tasks, including programming, mathematical reasoning, understanding long documents, summarization, and dialogue.\n\nIt boasts multi-modal capabilities, enabling it to process visual inputs, and supports dozens of languages, including over 80 coding languages. Additionally, it features function calling and agentic workflows.\n\nMistral Medium 3 is optimized for single-node inference, particularly for long-context applications. Its size allows it to achieve high throughput on a single node.",
    "order": 105,
    "flag": 1,
    "context_length": "",
    "input_images": "",
    "input_audios": "",
    "latency": 0,
    "throughput": 0,
    "usage": "",
    "usage_en": "",
    "modalities": "",
    "features": "",
    "tags": "",
    "types": "",
    "display_input": "",
    "display_output": "",
    "parameters": []
  },
  {
    "model": "Baichuan3-Turbo",
    "developer": "Baichuan",
    "developer_id": 20,
    "provider_id": 26,
    "model_name": "Baichuan3-Turbo",
    "model_ratio": 0.95,
    "cache_ratio": 1,
    "completion_ratio": 1,
    "img_price_config": "",
    "billing_config": "",
    "desc": "",
    "desc_en": "",
    "order": 0,
    "flag": 0,
    "context_length": "",
    "input_images": "",
    "input_audios": "",
    "latency": 0,
    "throughput": 0,
    "usage": "",
    "usage_en": "",
    "modalities": "",
    "features": "",
    "tags": "",
    "types": "",
    "display_input": "",
    "display_output": "",
    "parameters": []
  },
  {
    "model": "Baichuan3-Turbo-128k",
    "developer": "Baichuan",
    "developer_id": 20,
    "provider_id": 26,
    "model_name": "Baichuan3-Turbo-128k",
    "model_ratio": 1.9,
    "cache_ratio": 1,
    "completion_ratio": 1,
    "img_price_config": "",
    "billing_config": "",
    "desc": "",
    "desc_en": "",
    "order": 0,
    "flag": 0,
    "context_length": "",
    "input_images": "",
    "input_audios": "",
    "latency": 0,
    "throughput": 0,
    "usage": "",
    "usage_en": "",
    "modalities": "",
    "features": "",
    "tags": "",
    "types": "",
    "display_input": "",
    "display_output": "",
    "parameters": []
  }
]